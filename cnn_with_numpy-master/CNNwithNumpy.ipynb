{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于python基础库，搭建卷积神经网络，并用该网络进行手写字符识别\n",
    "\n",
    "#### 本程序主要基于python的numpy、math等基础函数库，完成了CNN训练的前向传播、后向传播、随机梯度下降更新等主要的函数功能；\n",
    "\n",
    "#### 并基于该程序，在MNIST数据集上进行手写字符识别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T16:54:43.869510Z",
     "end_time": "2023-04-19T16:54:44.136902Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import,division,print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from six.moves import xrange\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-19T16:54:48.951873Z",
     "end_time": "2023-04-19T16:54:48.970888Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE=28\n",
    "NUM_CHANNELS=1 \n",
    "PIXEL_DEPTH=255\n",
    "BATCH_SIZE=64\n",
    "NUM_LABELS=10\n",
    "VAL_SIZE=64\n",
    "MAX_EPOCHS = 2\n",
    "SEED=2330\n",
    "EVAL_FREQUENCY = 100\n",
    "EVAL_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入MNIST训练数据集，并将训练集划分出验证集\n",
    "\n",
    "具体函数请参看`data_helper.py`文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data,  ./data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mEOFError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m      9\u001B[0m validation_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5000\u001B[39m\n\u001B[1;32m---> 10\u001B[0m train_data,train_labels,validation_data,validation_labels \u001B[38;5;241m=\u001B[39m \u001B[43mload_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43mIMAGE_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mNUM_CHANNELS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPIXEL_DEPTH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m test_data,test_labels \u001B[38;5;241m=\u001B[39m load_test(IMAGE_SIZE, NUM_CHANNELS, PIXEL_DEPTH)\n\u001B[0;32m     13\u001B[0m train_labels \u001B[38;5;241m=\u001B[39m onehot(train_labels, \u001B[38;5;241m55000\u001B[39m)\n",
      "File \u001B[1;32mF:\\pycham\\PycharmProjects\\pythonProject\\cnn_with_numpy-master\\data_helper.py:37\u001B[0m, in \u001B[0;36mload_train\u001B[1;34m(validation_size, image_size, num_channels, pixel_depth)\u001B[0m\n\u001B[0;32m     34\u001B[0m train_data_filename \u001B[38;5;241m=\u001B[39m file_directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain-images-idx3-ubyte.gz\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     35\u001B[0m train_labels_filename \u001B[38;5;241m=\u001B[39m file_directory \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain-labels-idx1-ubyte.gz\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 37\u001B[0m train_data \u001B[38;5;241m=\u001B[39m \u001B[43mextract_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpixel_depth\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m train_labels \u001B[38;5;241m=\u001B[39m extract_labels(train_labels_filename,data_size)\n\u001B[0;32m     40\u001B[0m validation_data \u001B[38;5;241m=\u001B[39m train_data[:validation_size, \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m]\n",
      "File \u001B[1;32mF:\\pycham\\PycharmProjects\\pythonProject\\cnn_with_numpy-master\\data_helper.py:15\u001B[0m, in \u001B[0;36mextract_data\u001B[1;34m(filename, num_images, image_size, num_channels, pixel_depth)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m gzip\u001B[38;5;241m.\u001B[39mopen(filename) \u001B[38;5;28;01mas\u001B[39;00m bytestream:\n\u001B[0;32m     14\u001B[0m     bytestream\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;241m16\u001B[39m)\n\u001B[1;32m---> 15\u001B[0m     buf \u001B[38;5;241m=\u001B[39m \u001B[43mbytestream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_size\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mimage_size\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnum_images\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mnum_channels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     data \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfrombuffer(buf, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39muint8)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     17\u001B[0m     data \u001B[38;5;241m=\u001B[39m (data\u001B[38;5;241m-\u001B[39m(pixel_depth\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m2.0\u001B[39m)) \u001B[38;5;241m/\u001B[39m pixel_depth\n",
      "File \u001B[1;32mG:\\conda\\lib\\gzip.py:300\u001B[0m, in \u001B[0;36mGzipFile.read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    298\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01merrno\u001B[39;00m\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(errno\u001B[38;5;241m.\u001B[39mEBADF, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread() on write-only GzipFile object\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 300\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\conda\\lib\\_compression.py:68\u001B[0m, in \u001B[0;36mDecompressReader.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreadinto\u001B[39m(\u001B[38;5;28mself\u001B[39m, b):\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b) \u001B[38;5;28;01mas\u001B[39;00m view, view\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m byte_view:\n\u001B[1;32m---> 68\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbyte_view\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m         byte_view[:\u001B[38;5;28mlen\u001B[39m(data)] \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(data)\n",
      "File \u001B[1;32mG:\\conda\\lib\\gzip.py:506\u001B[0m, in \u001B[0;36m_GzipReader.read\u001B[1;34m(self, size)\u001B[0m\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    505\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buf \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 506\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompressed file ended before the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    507\u001B[0m                        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mend-of-stream marker was reached\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_add_read_data( uncompress )\n\u001B[0;32m    510\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(uncompress)\n",
      "\u001B[1;31mEOFError\u001B[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "from data_helper import load_train,load_test\n",
    "\n",
    "def onehot(targets, num):\n",
    "    result = np.zeros((num, 10))\n",
    "    for i in range(num):\n",
    "        result[i][targets[i]] = 1\n",
    "    return result\n",
    "\n",
    "validation_size = 5000\n",
    "train_data,train_labels,validation_data,validation_labels = load_train(validation_size,IMAGE_SIZE, NUM_CHANNELS, PIXEL_DEPTH)\n",
    "test_data,test_labels = load_test(IMAGE_SIZE, NUM_CHANNELS, PIXEL_DEPTH)\n",
    "\n",
    "train_labels = onehot(train_labels, 55000)\n",
    "validation_labels = onehot(validation_labels, validation_size)\n",
    "train_size = train_labels.shape[0]\n",
    "print(\"train data size: \",len(train_data))\n",
    "print(\"train data shape: \",train_data.shape)\n",
    "print(\"train label shape: \",train_labels.shape)\n",
    "print(\"test data shape: \", test_data.shape)\n",
    "print(\"test label shape: \", test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m----> 2\u001B[0m plt\u001B[38;5;241m.\u001B[39mimshow(\u001B[43mtrain_data\u001B[49m[index]\u001B[38;5;241m.\u001B[39mreshape((\u001B[38;5;241m28\u001B[39m,\u001B[38;5;241m28\u001B[39m)),cmap \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mcm\u001B[38;5;241m.\u001B[39mgray)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my is: \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(train_labels[index]))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "plt.imshow(train_data[index].reshape((28,28)),cmap = plt.cm.gray)\n",
    "print(\"y is: \"+str(train_labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置输入层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, layer_shape, k_size=5, k_num=32, strides=1, seed=2330, padding='SAME'):\n",
    "        self.input_shape = layer_shape\n",
    "        self.input_batch = layer_shape[0]\n",
    "        self.input_height = layer_shape[1]\n",
    "        self.input_width = layer_shape[2]\n",
    "        self.input_channels = layer_shape[3]\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.k_size = k_size\n",
    "        self.strides = strides\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        if (self.input_height-self.k_size) % self.strides != 0:\n",
    "            print(\"input tensor height can\\'t fit strides!\")\n",
    "        if (self.input_width-self.k_size) % self.strides != 0:\n",
    "            print(\"input tensor width can\\'t fit strides!\")\n",
    "        \n",
    "        self.padding = padding\n",
    "        if self.padding=='SAME':\n",
    "            self.output_batch = self.input_batch\n",
    "            self.output_height = self.input_height\n",
    "            self.output_width = self.input_width\n",
    "            self.output_channels = k_num\n",
    "            self.delta = np.zeros((self.output_batch, self.output_height, self.output_width, self.output_channels))\n",
    "        elif self.padding=='VALID':\n",
    "            self.output_batch = self.input_batch\n",
    "            self.output_height = (self.input_height-self.k_size) // self.strides + 1\n",
    "            self.output_width = (self.input_width-self.k_size) // self.strides + 1\n",
    "            self.output_channels = k_num\n",
    "            self.delta = np.zeros((self.output_batch, self.output_height, self.output_width, self.output_channels))\n",
    "        \n",
    "        self.output_shape = self.delta.shape\n",
    "        \n",
    "        weights_scale = math.sqrt(k_size*k_size*self.output_channels/2) # init filter weights with dividing weights_scale \n",
    "        self.weights = np.random.standard_normal((k_size, k_size, self.input_channels, self.output_channels)) / weights_scale\n",
    "        self.bias = np.random.standard_normal(self.output_channels) / weights_scale\n",
    "        \n",
    "        self.w_gradient = np.zeros(self.weights.shape)\n",
    "        self.b_gradient = np.zeros(self.bias.shape)\n",
    "        \n",
    "    def img2col(self, image, ksize, stride):\n",
    "        # image is a 4d tensor([batchsize, width ,height, channel])\n",
    "        image_col = []\n",
    "        for i in range(0, image.shape[1] - ksize + 1, stride):\n",
    "            for j in range(0, image.shape[2] - ksize + 1, stride):\n",
    "                col = image[:, i:i + ksize, j:j + ksize, :].reshape([-1])\n",
    "                image_col.append(col)\n",
    "        image_col = np.array(image_col)\n",
    "\n",
    "        return image_col\n",
    "        \n",
    "        \n",
    "    def forward(self,X):\n",
    "        col_weights = self.weights.reshape([-1, self.output_channels])\n",
    "        # padding\n",
    "        if self.padding == 'SAME':\n",
    "            X = np.pad(X, \n",
    "                ((0,0), (self.k_size//2,self.k_size//2), (self.k_size//2, self.k_size//2),(0,0)),\n",
    "                'constant', constant_values=(0,0))\n",
    "#         print(X.shape)\n",
    "        # convolution \n",
    "        self.col_image = []\n",
    "        conv_out = np.zeros(self.delta.shape)\n",
    "        for i in range(self.input_batch):\n",
    "            img_i = X[i][np.newaxis, :]\n",
    "            self.col_image_i = self.img2col(img_i, self.k_size, self.strides)\n",
    "            conv_out[i] = np.reshape(np.dot(self.col_image_i, col_weights) + self.bias, self.delta[0].shape)\n",
    "            self.col_image.append(self.col_image_i)\n",
    "        self.col_image = np.array(self.col_image)\n",
    "        return conv_out\n",
    "        \n",
    "    \n",
    "    def backward(self, delta, lr=0.0001, weight_decay=0.0004):\n",
    "        self.delta = delta\n",
    "        col_delta = np.reshape(delta, [self.input_batch, -1, self.output_channels])\n",
    "        \n",
    "        for i in range(self.input_batch):\n",
    "            self.w_gradient += np.dot(self.col_image[i].T, col_delta[i]).reshape(self.weights.shape)\n",
    "        self.b_gradient += np.sum(col_delta, axis=(0,1))\n",
    "        \n",
    "        if self.padding == 'SAME':\n",
    "            pad_delta = np.pad(self.delta, \n",
    "                                ((0,0), (self.k_size//2,self.k_size//2), (self.k_size//2, self.k_size//2),(0,0)),\n",
    "                                'constant', constant_values=(0,0))\n",
    "        else:\n",
    "            pad_delta = np.pad(self.delta, \n",
    "                                ((0, 0), (self.k_size - 1, self.k_size - 1), (self.k_size - 1, self.k_size - 1), (0, 0)),\n",
    "                                'constant', constant_values=0)\n",
    "            \n",
    "        flip_weights = np.flipud(np.fliplr(self.weights))\n",
    "        flip_weights = flip_weights.swapaxes(2, 3)\n",
    "        col_flip_weights = flip_weights.reshape([-1, self.input_channels])\n",
    "        col_pad_delta = np.array([self.img2col(pad_delta[i][np.newaxis, :], self.k_size, self.strides) for i in range(self.input_batch)])\n",
    "        delta_back = np.dot(col_pad_delta, col_flip_weights)\n",
    "        delta_back = np.reshape(delta_back, self.input_shape)\n",
    "        \n",
    "        # update weights\n",
    "        self.weights = (1-weight_decay)*self.weights - lr*self.w_gradient\n",
    "        self.bias = (1-weight_decay)*self.bias - lr*self.bias\n",
    "        self.w_gradient = np.zeros(self.weights.shape)\n",
    "        self.b_gradient = np.zeros(self.bias.shape)\n",
    "        return delta_back\n",
    "\n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330, padding='VALID')\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    \n",
    "    conv_back = conv.backward(conv_out, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "relu_out.shape:  (2, 26, 26, 2)\n",
      "relu_back.shape: (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# rule Activator\n",
    "class Relu:  \n",
    "    def __init__(self, input_shape):\n",
    "        self.delta = np.zeros(input_shape)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = self.input_shape\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def backward(self, delta):\n",
    "        delta[self.x<0] = 0\n",
    "        return delta\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330, padding='VALID')\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv_out.shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    relu_back = relu.backward(relu_out)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_pooling 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 26, 26, 2)\n",
      "relu_out.shape:  (2, 26, 26, 2)\n",
      "pool_out.shape:  (2, 13, 13, 2)\n",
      "pool_back.shape: (2, 26, 26, 2)\n",
      "relu_back.shape: (2, 26, 26, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class max_pool:\n",
    "    def __init__(self, input_shape, k_size=2, strides=2):\n",
    "        self.input_shape = input_shape\n",
    "        self.k_size = k_size\n",
    "        self.strides = strides\n",
    "        self.output_shape = [input_shape[0], input_shape[1] // self.strides, input_shape[2] // self.strides, input_shape[3]]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, w, h, c = x.shape\n",
    "        feature_w = w // self.strides\n",
    "        feature = np.zeros((b, feature_w, feature_w, c))\n",
    "        self.feature_mask = np.zeros((b, w, h, c))   # 记录最大池化时最大值的位置信息用于反向传播\n",
    "        for bi in range(b):\n",
    "            for ci in range(c):\n",
    "                for i in range(0,h,self.strides):\n",
    "                    for j in range(0,w, self.strides):\n",
    "                        feature[bi, i//self.strides, j//self.strides, ci] = np.max(\n",
    "                            x[bi,i:i+self.k_size,j:j+self.k_size,ci])\n",
    "                        index = np.argmax(x[bi, i:i+self.k_size, j:j+self.k_size,ci])\n",
    "                        self.feature_mask[bi, i+index//self.strides, j+index%self.strides, ci] = 1                    \n",
    "        return feature\n",
    "\n",
    "    def backward(self, delta):\n",
    "        return np.repeat(np.repeat(delta, self.strides, axis=1), self.strides, axis=2) * self.feature_mask\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330, padding='VALID')\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv_out.shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu_out.shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    pool_back = pool.backward(pool_out)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 28, 28, 2)\n",
      "relu_out.shape:  (2, 28, 28, 2)\n",
      "pool_out.shape:  (2, 14, 14, 2)\n",
      "flat_out.shape:  (2, 392)\n",
      "flat_back.shape: (2, 14, 14, 2)\n",
      "pool_back.shape: (2, 28, 28, 2)\n",
      "relu_back.shape: (2, 28, 28, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class flatten:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = [self.input_shape[0], self.input_shape[1] * self.input_shape[2] * self.input_shape[3]]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x.reshape([self.input_shape[0], self.input_shape[1] * self.input_shape[2] * self.input_shape[3]])\n",
    "        return y\n",
    "    def backward(self, y):\n",
    "        x = np.reshape(y, [self.input_shape[0], self.input_shape[1], self.input_shape[2], self.input_shape[3]])\n",
    "        return x\n",
    "\n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330, padding='SAME')\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv.output_shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu.output_shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    flat = flatten(pool.output_shape)\n",
    "    flat_out = flat.forward(pool_out)\n",
    "    print(\"flat_out.shape: \", flat_out.shape)\n",
    "    \n",
    "    flat_back = flat.backward(flat_out)\n",
    "    print(\"flat_back.shape:\", flat_back.shape)\n",
    "    \n",
    "    pool_back = pool.backward(flat_back)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全连接层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 28, 28, 2)\n",
      "relu_out.shape:  (2, 28, 28, 2)\n",
      "pool_out.shape:  (2, 14, 14, 2)\n",
      "flat_out.shape:  (2, 392)\n",
      "fc_out.shape: \t (2, 10)\n",
      "fc_back.shape: \t (2, 392)\n",
      "flat_back.shape: (2, 14, 14, 2)\n",
      "pool_back.shape: (2, 28, 28, 2)\n",
      "relu_back.shape: (2, 28, 28, 2)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class full_connection:\n",
    "    def __init__(self, input_shape, output_channels, seed=2330):\n",
    "        self.input_shape = input_shape\n",
    "        self.input_batch = input_shape[0]\n",
    "        self.input_length = input_shape[1]\n",
    "        self.output_channels = output_channels\n",
    "        \n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        weights_scale = math.sqrt(self.input_length/2) \n",
    "        self.weights = np.random.standard_normal((self.input_length,self.output_channels)) / weights_scale\n",
    "        self.bias = np.random.standard_normal(self.output_channels) / weights_scale\n",
    "        \n",
    "        self.output_shape = [self.input_batch, self.output_channels]\n",
    "        self.w_gradient = np.zeros(self.weights.shape)\n",
    "        self.b_gradient = np.zeros(self.bias.shape)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        y = np.dot(self.x, self.weights) +self.bias\n",
    "        return y\n",
    "    \n",
    "    def backward(self, delta, lr=0.0001, weight_decay=0.0004):\n",
    "        delta_back = np.dot(delta, self.weights.T)\n",
    "        self.w_gradient = np.dot(self.x.T, delta)\n",
    "        self.b_gradient = np.sum(delta, axis=0)\n",
    "        \n",
    "        self.weights = (1-weight_decay)*self.weights - lr*self.w_gradient\n",
    "        self.bias = (1-weight_decay)*self.bias - lr*self.bias\n",
    "        \n",
    "        self.w_gradient = np.zeros(self.weights.shape)\n",
    "        self.b_gradient = np.zeros(self.bias.shape)\n",
    "        return delta_back\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=3, k_num=2, strides=1, seed=2330, padding='SAME')\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv.output_shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu.output_shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    flat = flatten(pool.output_shape)\n",
    "    flat_out = flat.forward(pool_out)\n",
    "    print(\"flat_out.shape: \", flat_out.shape)\n",
    "    \n",
    "    fc = full_connection(flat.output_shape,10,seed=SEED)\n",
    "    fc_out = fc.forward(flat_out)\n",
    "    print(\"fc_out.shape: \\t\", fc_out.shape)\n",
    "    \n",
    "    fc_back = fc.backward(fc_out,lr=0.0001)\n",
    "    print(\"fc_back.shape: \\t\",fc_back.shape)\n",
    "    \n",
    "    flat_back = flat.backward(fc_back)\n",
    "    print(\"flat_back.shape:\", flat_back.shape)\n",
    "    \n",
    "    pool_back = pool.backward(flat_back)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape: \t (2, 28, 28, 1)\n",
      "conv_out.shape:  (2, 28, 28, 32)\n",
      "relu_out.shape:  (2, 28, 28, 32)\n",
      "pool_out.shape:  (2, 14, 14, 32)\n",
      "flat_out.shape:  (2, 6272)\n",
      "fc_out.shape: \t (2, 10)\n",
      "pred.shape: \t (2, 10)\n",
      "loss:  \t\t 2.320875261985801\n",
      "loss_back.shape: (2, 10)\n",
      "fc_back.shape: \t (2, 6272)\n",
      "flat_back.shape: (2, 14, 14, 32)\n",
      "pool_back.shape: (2, 28, 28, 32)\n",
      "relu_back.shape: (2, 28, 28, 32)\n",
      "conv_back.shape: (2, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.input_batch = input_shape[0]\n",
    "        self.class_num = input_shape[1]\n",
    "        \n",
    "        self.softmax = np.zeros(self.input_shape)\n",
    "        self.delta = np.zeros(self.input_shape)\n",
    "        \n",
    "    def cal_loss(self, x, label):\n",
    "        self.prediction(x)\n",
    "        loss = 0\n",
    "        for i in range(self.input_batch):\n",
    "            loss -= np.sum(np.log(self.softmax[i]) * label[i])\n",
    "        loss /= self.input_batch\n",
    "        return loss\n",
    "    \n",
    "    def prediction(self, x):\n",
    "        for i in range(self.input_batch):\n",
    "            pred_tmp = x[i, :] - np.max(x[i, :])\n",
    "            pred_tmp = np.exp(pred_tmp)\n",
    "            self.softmax[i] = pred_tmp/np.sum(pred_tmp)\n",
    "        return self.softmax\n",
    "    \n",
    "    def backward(self, label):\n",
    "        for i in range(self.input_batch):\n",
    "            self.delta[i] = self.softmax[i] - label[i]\n",
    "        return self.delta\n",
    "    \n",
    "def test_():\n",
    "    index = 1\n",
    "    img = train_data[index:index+2, ...]\n",
    "    y = train_labels[index:index+2,...]\n",
    "    print(\"img.shape: \\t\", img.shape)\n",
    "    \n",
    "    conv = Convolution(img.shape, k_size=5, k_num=32, strides=1, seed=2330, padding='SAME')\n",
    "    conv_out = conv.forward(img)\n",
    "    print(\"conv_out.shape: \", conv_out.shape)\n",
    "\n",
    "    relu=Relu(conv.output_shape)\n",
    "    relu_out = relu.forward(conv_out)\n",
    "    print(\"relu_out.shape: \", relu_out.shape)\n",
    "    \n",
    "    pool = max_pool(relu.output_shape,2,2)\n",
    "    pool_out = pool.forward(relu_out)\n",
    "    print(\"pool_out.shape: \", pool_out.shape)\n",
    "    \n",
    "    flat = flatten(pool.output_shape)\n",
    "    flat_out = flat.forward(pool_out)\n",
    "    print(\"flat_out.shape: \", flat_out.shape)\n",
    "    \n",
    "    fc = full_connection(flat.output_shape,10,seed=SEED)\n",
    "    fc_out = fc.forward(flat_out)\n",
    "    print(\"fc_out.shape: \\t\", fc_out.shape)\n",
    "    \n",
    "    softmax = Softmax(fc.output_shape)\n",
    "    \n",
    "    pred = softmax.prediction(fc_out)\n",
    "    print(\"pred.shape: \\t\", pred.shape)\n",
    "    \n",
    "    loss = softmax.cal_loss(fc_out, y)\n",
    "    print(\"loss:  \\t\\t\", loss)\n",
    "    \n",
    "    loss_back = softmax.backward(label=y)\n",
    "    print(\"loss_back.shape:\", loss_back.shape)\n",
    "    \n",
    "    fc_back = fc.backward(loss_back,lr=0.0001)\n",
    "    print(\"fc_back.shape: \\t\",fc_back.shape)\n",
    "    \n",
    "    flat_back = flat.backward(fc_back)\n",
    "    print(\"flat_back.shape:\", flat_back.shape)\n",
    "    \n",
    "    pool_back = pool.backward(flat_back)\n",
    "    print(\"pool_back.shape:\", pool_back.shape)\n",
    "    \n",
    "    relu_back = relu.backward(pool_back)\n",
    "    print(\"relu_back.shape:\", relu_back.shape)\n",
    "    \n",
    "    conv_back = conv.backward(relu_back, lr=0.0001)\n",
    "    print(\"conv_back.shape:\", conv_back.shape)\n",
    "    \n",
    "    \n",
    "test_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建CNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self,num_labels=10, batch_size=64, image_size=28, num_channels=1, seed=66478):\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.num_channels = num_channels\n",
    "        self.seed = seed\n",
    "        self.num_labels=num_labels\n",
    "        self.net_builder()\n",
    "        \n",
    "    def net_builder(self):\n",
    "        self.conv1 = Convolution([self.batch_size, self.image_size, self.image_size, self.num_channels], k_size=5, k_num=6, strides=1, seed=2330, padding='VALID')\n",
    "        self.relu1 = Relu(self.conv1.output_shape)\n",
    "        self.pool1 = max_pool(self.relu1.output_shape, 2,2)\n",
    "            \n",
    "        self.conv2 = Convolution(self.pool1.output_shape, k_size=5, k_num=16, strides=1, seed=2330, padding='VALID')\n",
    "        self.relu2 = Relu(self.conv2.output_shape)\n",
    "        self.pool2 = max_pool(self.relu2.output_shape, 2,2)\n",
    "            \n",
    "        self.flat = flatten(self.pool2.output_shape)\n",
    "        self.fc1 = full_connection(self.flat.output_shape,512,seed=SEED)\n",
    "        self.relu3 = Relu(self.fc1.output_shape)\n",
    "            \n",
    "        self.fc2 = full_connection(self.relu3.output_shape,10,seed=SEED)\n",
    "            \n",
    "        self.softmax = Softmax(self.fc2.output_shape)\n",
    "        \n",
    "    def cal_forward(self,x):\n",
    "        conv1_out = self.conv1.forward(x)\n",
    "        relu1_out = self.relu1.forward(conv1_out)\n",
    "        pool1_out = self.pool1.forward(relu1_out)\n",
    "        \n",
    "        conv2_out = self.conv2.forward(pool1_out)\n",
    "        relu2_out = self.relu2.forward(conv2_out)\n",
    "        pool2_out = self.pool2.forward(relu2_out)\n",
    "        \n",
    "        flat_out = self.flat.forward(pool2_out)\n",
    "        fc1_out = self.fc1.forward(flat_out)\n",
    "        relu3_out = self.relu3.forward(fc1_out)\n",
    "        \n",
    "        fc2_out = self.fc2.forward(relu3_out)\n",
    "\n",
    "        pred = self.softmax.prediction(fc2_out)\n",
    "        return np.argmax(pred,axis=1)\n",
    "\n",
    "    def fit(self, x, y, lr):\n",
    "        conv1_out = self.conv1.forward(x)\n",
    "        relu1_out = self.relu1.forward(conv1_out)\n",
    "        pool1_out = self.pool1.forward(relu1_out)\n",
    "        \n",
    "        conv2_out = self.conv2.forward(pool1_out)\n",
    "        relu2_out = self.relu2.forward(conv2_out)\n",
    "        pool2_out = self.pool2.forward(relu2_out)\n",
    "        \n",
    "        flat_out = self.flat.forward(pool2_out)\n",
    "        fc1_out = self.fc1.forward(flat_out)\n",
    "        relu3_out = self.relu3.forward(fc1_out)\n",
    "        \n",
    "        fc2_out = self.fc2.forward(relu3_out)\n",
    "\n",
    "        pred = self.softmax.prediction(fc2_out)\n",
    "        loss = self.softmax.cal_loss(fc2_out, y)\n",
    "        \n",
    "        loss_back = self.softmax.backward(label=y)\n",
    "        fc2_back = self.fc2.backward(loss_back, lr=lr)\n",
    "        relu3_back = self.relu3.backward(fc2_back)\n",
    "        fc1_back = self.fc1.backward(relu3_back, lr=lr)\n",
    "        flat_back = self.flat.backward(fc1_back)\n",
    "        poo2_back = self.pool2.backward(flat_back)\n",
    "        relu2_back = self.relu2.backward(poo2_back)\n",
    "        conv2_back = self.conv2.backward(relu2_back)\n",
    "        pool1_back = self.pool1.backward(conv2_back)\n",
    "        relu1_back = self.relu1.backward(pool1_back)\n",
    "        conv1__back = self.conv1.backward(relu1_back)\n",
    "        return loss, np.argmax(pred,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "def cal_acc(predictions, labels):\n",
    "    return (100.0 * np.sum(predictions == labels) /predictions.shape[0]) \n",
    "\n",
    "def eval_in_batches(data,cnn):\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "        raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.zeros(size)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            x = data[begin:end, ...]\n",
    "            predictions[begin:end] = cnn.cal_forward(x)\n",
    "        else:\n",
    "            x = data[-EVAL_BATCH_SIZE:, ...]\n",
    "            batch_predictions = cnn.cal_forward(x)\n",
    "            predictions[begin:] = batch_predictions[begin - size:]\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 (epoch 0.00)\n",
      "Minibatch loss: 2.387\n",
      "Minibatch acc:  9.4%\n",
      "Validation error: 8.7%\n",
      "Step 50 (epoch 0.06)\n",
      "Minibatch loss: 0.761\n",
      "Minibatch acc:  79.7%\n",
      "Validation error: 81.0%\n",
      "Step 100 (epoch 0.12)\n",
      "Minibatch loss: 0.253\n",
      "Minibatch acc:  92.2%\n",
      "Validation error: 89.1%\n",
      "Step 150 (epoch 0.17)\n",
      "Minibatch loss: 0.436\n",
      "Minibatch acc:  81.2%\n",
      "Validation error: 85.4%\n",
      "Step 200 (epoch 0.23)\n",
      "Minibatch loss: 0.328\n",
      "Minibatch acc:  92.2%\n",
      "Validation error: 92.2%\n",
      "Step 250 (epoch 0.29)\n",
      "Minibatch loss: 0.175\n",
      "Minibatch acc:  95.3%\n",
      "Validation error: 94.3%\n",
      "Step 300 (epoch 0.35)\n",
      "Minibatch loss: 0.211\n",
      "Minibatch acc:  92.2%\n",
      "Validation error: 93.9%\n",
      "Step 350 (epoch 0.41)\n",
      "Minibatch loss: 0.122\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 94.9%\n",
      "Step 400 (epoch 0.47)\n",
      "Minibatch loss: 0.307\n",
      "Minibatch acc:  90.6%\n",
      "Validation error: 95.2%\n",
      "Step 450 (epoch 0.52)\n",
      "Minibatch loss: 0.125\n",
      "Minibatch acc:  96.9%\n",
      "Validation error: 94.5%\n",
      "Step 500 (epoch 0.58)\n",
      "Minibatch loss: 0.255\n",
      "Minibatch acc:  92.2%\n",
      "Validation error: 94.2%\n",
      "Step 550 (epoch 0.64)\n",
      "Minibatch loss: 0.161\n",
      "Minibatch acc:  93.8%\n",
      "Validation error: 95.6%\n",
      "Step 600 (epoch 0.70)\n",
      "Minibatch loss: 0.092\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 96.4%\n",
      "Step 650 (epoch 0.76)\n",
      "Minibatch loss: 0.071\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 96.2%\n",
      "Step 700 (epoch 0.81)\n",
      "Minibatch loss: 0.114\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 95.7%\n",
      "Step 750 (epoch 0.87)\n",
      "Minibatch loss: 0.120\n",
      "Minibatch acc:  95.3%\n",
      "Validation error: 96.6%\n",
      "Step 800 (epoch 0.93)\n",
      "Minibatch loss: 0.115\n",
      "Minibatch acc:  96.9%\n",
      "Validation error: 96.2%\n",
      "Step 850 (epoch 0.99)\n",
      "Minibatch loss: 0.224\n",
      "Minibatch acc:  92.2%\n",
      "Validation error: 95.4%\n",
      "Step 900 (epoch 1.05)\n",
      "Minibatch loss: 0.064\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 96.1%\n",
      "Step 950 (epoch 1.11)\n",
      "Minibatch loss: 0.051\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 97.0%\n",
      "Step 1000 (epoch 1.16)\n",
      "Minibatch loss: 0.189\n",
      "Minibatch acc:  93.8%\n",
      "Validation error: 96.8%\n",
      "Step 1050 (epoch 1.22)\n",
      "Minibatch loss: 0.057\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 96.4%\n",
      "Step 1100 (epoch 1.28)\n",
      "Minibatch loss: 0.036\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 96.5%\n",
      "Step 1150 (epoch 1.34)\n",
      "Minibatch loss: 0.149\n",
      "Minibatch acc:  93.8%\n",
      "Validation error: 96.8%\n",
      "Step 1200 (epoch 1.40)\n",
      "Minibatch loss: 0.119\n",
      "Minibatch acc:  93.8%\n",
      "Validation error: 96.7%\n",
      "Step 1250 (epoch 1.45)\n",
      "Minibatch loss: 0.113\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 96.8%\n",
      "Step 1300 (epoch 1.51)\n",
      "Minibatch loss: 0.059\n",
      "Minibatch acc:  96.9%\n",
      "Validation error: 96.4%\n",
      "Step 1350 (epoch 1.57)\n",
      "Minibatch loss: 0.058\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 96.9%\n",
      "Step 1400 (epoch 1.63)\n",
      "Minibatch loss: 0.090\n",
      "Minibatch acc:  96.9%\n",
      "Validation error: 97.2%\n",
      "Step 1450 (epoch 1.69)\n",
      "Minibatch loss: 0.092\n",
      "Minibatch acc:  98.4%\n",
      "Validation error: 97.4%\n",
      "Step 1500 (epoch 1.75)\n",
      "Minibatch loss: 0.159\n",
      "Minibatch acc:  95.3%\n",
      "Validation error: 97.5%\n",
      "Step 1550 (epoch 1.80)\n",
      "Minibatch loss: 0.046\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 97.0%\n",
      "Step 1600 (epoch 1.86)\n",
      "Minibatch loss: 0.064\n",
      "Minibatch acc:  96.9%\n",
      "Validation error: 97.1%\n",
      "Step 1650 (epoch 1.92)\n",
      "Minibatch loss: 0.023\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 96.9%\n",
      "Step 1700 (epoch 1.98)\n",
      "Minibatch loss: 0.019\n",
      "Minibatch acc:  100.0%\n",
      "Validation error: 97.2%\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(num_labels=10, batch_size=64, image_size=28, num_channels=1, seed=SEED)\n",
    "learning_rate = 0.01\n",
    "\n",
    "start_time = time.time()\n",
    "for step in xrange(int(MAX_EPOCHS * train_size) // BATCH_SIZE):\n",
    "    offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "    batch_x = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "    batch_y = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "\n",
    "    loss, predictions = cnn.fit(batch_x,batch_y,learning_rate)\n",
    "#     acc = cal_acc(predictions, np.argmax(batch_y, axis=1))\n",
    "#     print('Step %d (epoch %.2f)' % (step, float(step) * BATCH_SIZE / train_size))\n",
    "#     print('Minibatch loss: %.3f' % loss)\n",
    "#     print('Minibatch acc:  %.1f%%' % acc)\n",
    "    \n",
    "    if step % EVAL_FREQUENCY == 0:\n",
    "        acc = cal_acc(predictions, np.argmax(batch_y, axis=1))\n",
    "        print('Step %d (epoch %.2f)' % (step, float(step) * BATCH_SIZE / train_size))\n",
    "        print('Minibatch loss: %.3f' % loss)\n",
    "        print('Minibatch acc:  %.1f%%' % acc)\n",
    "        val_predictions = eval_in_batches(validation_data[0:1000, ...],cnn)\n",
    "        val_acc = cal_acc(val_predictions, np.argmax(validation_labels[0:1000], axis=1))\n",
    "        print('Validation error: %.1f%%' % val_acc)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试过程，输出测试集准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data acc: 97.7%\n"
     ]
    }
   ],
   "source": [
    "test_predictions = eval_in_batches(test_data, cnn)\n",
    "test_acc = cal_acc(test_predictions, test_labels)\n",
    "print(\"test data acc: %.1f%%\" % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "- [https://zhuanlan.zhihu.com/c_162633442](https://zhuanlan.zhihu.com/c_162633442)\n",
    "- [https://www.cnblogs.com/qxcheng/p/11729773.html](https://www.cnblogs.com/qxcheng/p/11729773.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
